{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = 'ipl_model.h5'\n",
    "\n",
    "try:\n",
    "    # Load the SavedModel\n",
    "    model = tf.keras.models.load_model(saved_model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{saved_model_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(list_of_comments):\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    # Iterate over each sentence in the corpus\n",
    "    for sentence in list_of_comments:\n",
    "        # Tokenize the sentence and convert to lowercase\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "        \n",
    "        word_vectors = []\n",
    "        # Iterate over each word in the sentence\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                # Retrieve the word vector from the Word2Vec model\n",
    "                word_vector = word2vec_model.wv[token]\n",
    "                \n",
    "                # Append the word vector to the list of word vectors\n",
    "                word_vectors.append(word_vector)\n",
    "            except KeyError:\n",
    "                # Handle out-of-vocabulary words\n",
    "        \n",
    "                pass\n",
    "        # If there are word vectors for the sentence\n",
    "        if word_vectors:\n",
    "            # word found -> mean of the vector\n",
    "            sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            # no word -> 0's\n",
    "            sentence_vector = np.zeros(word2vec_model.vector_size)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    \n",
    "    # list -> np array\n",
    "    sentence_vectors = np.array(sentence_vectors)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {8: 'Wicket',\n",
    " 1: 'One Run',\n",
    " 4: 'Four Runs',\n",
    " 6: 'Six Runs',\n",
    " 0: 'Zero Runs',\n",
    " 2: 'Two Runs',\n",
    " 7: 'Seven Runs',\n",
    " 5: 'Five Runs',\n",
    " 3: 'Three Runs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(text):\n",
    "    # Converting text to preprocessing\n",
    "    text_vectorization = create_vector([text])\n",
    "    # Predicting the given text\n",
    "    predicted_value = model.predict(text_vectorization)\n",
    "    print(label_mapping[predicted_value.argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" short of good length ball, pitching on leg stump, Rilee Rossouw gets the hang of the bounce and smashes the pull towards deep square leg for another full montyy! \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Six Runs\n"
     ]
    }
   ],
   "source": [
    "predict_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
